# ðŸ”´ BART

* **Architecture:** Encoderâ€“Decoder (like a seq2seq transformer).
* **Training objective:** Denoising autoencoder.
  * Input: Corrupted sentence.
  * Output: Original sentence.
* **For summarization:**
  * Encoder â†’ encodes input document.
  * Decoder â†’ generates summary.
* **Strength:** Very good for long-form abstractive summarization.
