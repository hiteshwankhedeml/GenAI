# ğŸ”´ T5

* **Architecture:** Encoderâ€“Decoder (similar to BART).
* **Training objective:** â€œText-to-Text Transfer Transformer.â€
  * Everything framed as a text-to-text problem.
  * Example: `"summarize: <document>" â†’ <summary>`
* **For summarization:** You prepend task prefix `"summarize:"`.
* **Strength:** General-purpose, handles multiple NLP tasks.
