# 🔴 T5

* **Architecture:** Encoder–Decoder (similar to BART).
* **Training objective:** “Text-to-Text Transfer Transformer.”
  * Everything framed as a text-to-text problem.
  * Example: `"summarize: <document>" → <summary>`
* **For summarization:** You prepend task prefix `"summarize:"`.
* **Strength:** General-purpose, handles multiple NLP tasks.
