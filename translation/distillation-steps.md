# ðŸŸ¢ Distillation Steps

* **Get batch of parallel data** (src, tgt).
* **Run teacher forward pass** â†’ get teacher logits â†’ convert to soft probabilities with temperature.
* **Run student forward pass** â†’ get student logits â†’ convert to log-soft probabilities.
* **Compute KL divergence** between teacher probabilities and student probabilities.
* **Optionally compute CE loss** against the true target tokens.
* **Combine losses** (e.g., 70% KL + 30% CE).
* **Backprop + optimize** student weights.
