# ðŸŸ¢ IndicTrans

* <mark style="color:purple;background-color:purple;">**IndicTrans uses 6 encoder and decoder layers, input embeddings of size 1536**</mark> with 16 attention heads and feedforward dimension of 4096 with total number of parameters of 434M
* Specifically built for Indian languages
* IndicTrans achieved higher BLEU scores on multiple Indic translation tasks compared to mBART, M2M-100, GNMT baselines
* <mark style="color:purple;background-color:purple;">**Direct Indicâ†”Indic translation**</mark>
* <mark style="color:purple;background-color:purple;">**Most models required English as a pivot.**</mark>
* <mark style="color:purple;background-color:purple;">**IndicTrans supported direct translation between Indic languages, reducing error propagation.**</mark>

