# BERT - PreTraining

**Masked Language Modelling:**

*

    <figure><img src=".gitbook/assets/image (1) (1).png" alt=""><figcaption></figcaption></figure>
* <mark style="color:purple;background-color:purple;">**Pick any random words in inside the sentence and mask it**</mark>
* <mark style="color:purple;background-color:purple;">**This words we are going to predict while training the model**</mark>
* <mark style="color:purple;background-color:purple;">**\[CLS]  token is represent that it needs to do classification of the sentence**</mark>
*   <mark style="color:purple;background-color:purple;">**\[SEP] to separate 2 sentences**</mark>

    <figure><img src=".gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

**Next Sentence Prediction:**

* Based on sentence â‡’ Predict the next sentence



Labelled data like sentiment text and label
