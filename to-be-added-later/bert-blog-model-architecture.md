# ðŸŸ¢ BERT Blog - Model Architecture

The paper presents two model sizes for BERT:

* <mark style="color:purple;background-color:purple;">**BERT BASE**</mark> â€“ Comparable in size to the OpenAI Transformer in order to compare performance
* <mark style="color:purple;background-color:purple;">**BERT LARGE**</mark> â€“ A ridiculously huge model which achieved the state of the art results reported in the paper
* <mark style="color:purple;background-color:purple;">**Both BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) â€“ 12 for the Base version, and 24 for the Large version.**</mark>&#x20;
* These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively)
