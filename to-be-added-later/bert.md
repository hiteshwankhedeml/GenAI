# ðŸŸ¢ BERT

* <mark style="color:purple;background-color:purple;">**Bidirectional Encoder Representation from Transformer**</mark>
* <mark style="color:purple;background-color:purple;">**Encoder based architecture**</mark>
* <mark style="color:purple;background-color:purple;">**Bidirectional means it is going to process the data in both the directions to self attention layer**</mark>
* We have positional encoding also
* Trained on massive wikipedia data
* Layer Normalization and Feed Forward
* 2 models
  * Small model having 12 layers â‡’ Hidden layer size in Feed forward NN â‡’ 768 â‡’ 110M parameters
  * Large model having 24 layers â‡’ Hidden layer size in Feed forward NN â‡’ 1024 â‡’ 340M parameters
*

    <figure><img src=".gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>
* BERT was mainly trained for text classification
* It was used for summarization also
* Base model can be fine tuned for any task
* In BERT 2 stages were involved
  * Unsupervised pre training
    * Masked Language Modelling
    * Next sentence prediction
  * Supervised fine tuning
* Huge data from books, wikipedia, reddit, facebook etc.
