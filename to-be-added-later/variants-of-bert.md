# ðŸŸ¢ Variants of BERT

| **Variant**                                                               | **Category**                                                                            | **Description / Purpose**                                                                                                       | **Use Cases**                                                                                                    |
| ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
| <mark style="color:purple;background-color:purple;">**RoBERTa**</mark>    | <mark style="color:purple;background-color:purple;">**General Improvement**</mark>      | <mark style="color:purple;background-color:purple;">**Trained longer on more data, without NSP for better performance.**</mark> | <mark style="color:purple;background-color:purple;">**Text classification, QA, NLI**</mark>                      |
| **ALBERT**                                                                | General Improvement                                                                     | Lightweight with parameter sharing and factorized embeddings.                                                                   | QA, classification, low-resource environments                                                                    |
| **ELECTRA**                                                               | General Improvement                                                                     | Trains efficiently using replaced token detection instead of masking.                                                           | Classification, QA, language understanding                                                                       |
| **DeBERTa**                                                               | General Improvement                                                                     | Improves attention with disentangled representations and relative positions.                                                    | QA, NLI, GLUE/SuperGLUE leaderboards                                                                             |
| **SpanBERT**                                                              | Task-Specific (QA)                                                                      | Better at predicting and modeling spans in text.                                                                                | QA, coreference resolution                                                                                       |
| <mark style="color:purple;background-color:purple;">**LUKE**</mark>       | <mark style="color:purple;background-color:purple;">**Task-Specific (NER, RE)**</mark>  | <mark style="color:purple;background-color:purple;">**Entity-aware model that incorporates entity embeddings.**</mark>          | <mark style="color:purple;background-color:purple;">**Named Entity Recognition, Relation Extraction**</mark>     |
| **CodeBERT**                                                              | Task-Specific (Code)                                                                    | Trained on source code + natural language pairs.                                                                                | Code search, code summarization, code generation                                                                 |
| **BioBERT**                                                               | Domain-Specific (Medical)                                                               | Biomedical text from PubMed and PMC.                                                                                            | Biomedical NER, QA, relation extraction                                                                          |
| **ClinicalBERT**                                                          | Domain-Specific (Medical)                                                               | Tuned on clinical notes from MIMIC-III.                                                                                         | Clinical text analysis, patient record mining                                                                    |
| **SciBERT**                                                               | Domain-Specific (Science)                                                               | Trained on scientific papers across multiple fields.                                                                            | Scientific text mining, NER                                                                                      |
| **FinBERT**                                                               | Domain-Specific (Finance)                                                               | Trained on financial data such as analyst reports and news.                                                                     | Financial sentiment analysis, classification                                                                     |
| **LegalBERT**                                                             | Domain-Specific (Legal)                                                                 | Trained on legal corpora like contracts and case law.                                                                           | Legal text classification, case analysis                                                                         |
| <mark style="color:purple;background-color:purple;">**mBERT**</mark>      | <mark style="color:purple;background-color:purple;">**Multilingual**</mark>             | <mark style="color:purple;background-color:purple;">**Trained on Wikipedia in 104 languages.**</mark>                           | <mark style="color:purple;background-color:purple;">**Cross-lingual tasks, translation, multilingual QA**</mark> |
| <mark style="color:purple;background-color:purple;">**XLM-R**</mark>      | <mark style="color:purple;background-color:purple;">**Multilingual**</mark>             | <mark style="color:purple;background-color:purple;">**RoBERTa-based multilingual model trained on more data.**</mark>           | <mark style="color:purple;background-color:purple;">**Multilingual classification, QA, NLI**</mark>              |
| <mark style="color:purple;background-color:purple;">**DistilBERT**</mark> | <mark style="color:purple;background-color:purple;">**Compression / Efficiency**</mark> | <mark style="color:purple;background-color:purple;">**40% smaller and 60% faster, retains 97% of BERTâ€™s performance.**</mark>   | <mark style="color:purple;background-color:purple;">**Real-time NLP tasks, mobile deployment**</mark>            |
| **TinyBERT**                                                              | Compression / Efficiency                                                                | Compressed version using knowledge distillation.                                                                                | Low-latency applications, mobile NLP                                                                             |
| **MobileBERT**                                                            | Compression / Efficiency                                                                | Optimized architecture for mobile and edge devices.                                                                             | On-device NLP (phones, IoT)                                                                                      |
| **MiniLM**                                                                | Compression / Efficiency                                                                | Small and fast; distilled with self-attention alignment.                                                                        | Search, semantic similarity, mobile deployment                                                                   |
