# ðŸŸ  Llama

* [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama)
* Research paper in <mark style="color:purple;background-color:purple;">**2023**</mark>
* <mark style="color:purple;background-color:purple;">**Decoder based architecture**</mark>
* Parameters from 7 billions to 70 billions
* Latest model is having 401B parameters
* <mark style="color:purple;background-color:purple;">**Optimized for dialog use case**</mark>
* <mark style="color:purple;background-color:purple;">**Developed by Meta as competitor of OpenAI**</mark>
* <mark style="color:purple;background-color:purple;">**Open source model**</mark>
* New positional encoding introduced â‡’ ROPE
* Rotary positional embedding â‡’ this is for capturing longer sentences
* In supervised fine tuning, they did dialog specific training ( Question and Answer form)
