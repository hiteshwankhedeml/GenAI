# BERT - PreTraining

**Masked Language Modelling:**

*

    <figure><img src=".gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>
* Pick any random words in inside the sentence and mask it
* This words we are going to predict while training the model
* \[CLS]  token is represent that it needs to do classification of the sentence
*   \[SEP] to separate 2 sentences

    <figure><img src=".gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

**Next Sentence Prediction:**

* Based on sentence â‡’ Predict the next sentence



Labelled data like sentiment text and label
