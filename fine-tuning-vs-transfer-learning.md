# Fine Tuning vs Transfer Learning

#### **Fine-Tuning vs Transfer Learning: Side-by-Side Comparison**

| Feature                                      | **Transfer Learning (Feature Extraction)**                                                                                                                  | **Fine-Tuning**                                                                                                                                     |
| -------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Definition**                               | <mark style="color:purple;background-color:purple;">**Uses a pre-trained model as a fixed feature extractor. Only the last few layers are trained.**</mark> | <mark style="color:purple;background-color:purple;">**A pre-trained model is further trained on new data, with some or all layers updated.**</mark> |
| **Layers Trained**                           | Only new layers (e.g., classifier on top). The pre-trained model remains frozen.                                                                            | Some or all pre-trained layers are updated along with new layers.                                                                                   |
| **Computational Cost**                       | Lower, since fewer parameters are trained.                                                                                                                  | Higher, as more layers are updated.                                                                                                                 |
| **Data Requirement**                         | Works well with small datasets.                                                                                                                             | Requires a larger dataset for effective learning.                                                                                                   |
| **Flexibility**                              | Limited, as the model only extracts features without modification.                                                                                          | More flexible, as the model can adapt better to the new task.                                                                                       |
| **Training Time**                            | Faster, since only a few layers are trained.                                                                                                                | Slower, as more layers require optimization.                                                                                                        |
| **Similarity to Pre-trained Task**           | Works well even if the new task is different from the original training task.                                                                               | Works best when the new task is similar to the original training task.                                                                              |
| **Risk of Overfitting**                      | Lower, as fewer parameters are trained.                                                                                                                     | Higher, especially with small datasets, since more parameters are tuned.                                                                            |
| **Example (BERT for Resume Classification)** | <mark style="color:purple;background-color:purple;">**Use BERT embeddings and train a classifier on top**</mark> (e.g., Logistic Regression, MLP).          | <mark style="color:purple;background-color:purple;">**Unfreeze BERT layers and fine-tun**</mark>**e** on the resume dataset for better adaptation.  |
