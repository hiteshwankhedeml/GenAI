# ViLT

* Vision and image language transformer
* [https://arxiv.org/pdf/2102.03334](https://arxiv.org/pdf/2102.03334)
* Image ⇒ Transformer ⇒ Get description of image
* Single encoder used
* 1D of image + Embedding of text will be passed to self attention layer
* That's why this is called vision image transformer
*

    <figure><img src=".gitbook/assets/image (20).png" alt=""><figcaption></figcaption></figure>
