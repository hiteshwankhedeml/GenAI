# Llama

* [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama)
* Research paper in 2023
* Decoder based architecture
* Parameters from 7 billions to 70 billions
* Latest model is having 401B parameters
* Optimized for dialog use case
* Developed by Meta as competitor of OpenAI
* Open source model
* New positional encoding introduced ⇒ ROPE
* Rotary positional embedding ⇒ this is for capturing longer sentences
* In supervised fine tuning, they did dialog specific training ( Question and Answer form)
